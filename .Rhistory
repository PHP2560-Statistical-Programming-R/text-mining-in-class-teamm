<<<<<<< HEAD
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
or(START, SPC) %R% feature.word %R% or(END, SPC))
}
}
#train classifier
model <- multinom(as.factor(Topic.Code) ~ . - Article_ID - Date - Title - Subject, data = model.data)
#training set accuracy
accuracy=mean(predict(model, newdata = model.data)==model.data$Topic.Code)
library(rebus)
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
or(START, SPC) %R% feature.word %R% or(END, SPC))
}
}
?START
or(START, SPC)
install.packages(rebus)
install.packages("rebus")
install.packages("rebus")
install.packages("dplyr")
install.packages("RTextTools")
install.packages("dplyr")
install.packages("tidytext")
install.packages(ggplot2)
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("caTools")
install.packages("stringr")
install.packages("foreign")
install.packages("nnet")
install.packages("magrittr")
=======
<<<<<<< HEAD
#Statistical analysis
positives$n_notpos <- positives$case_total-positives$n
prop_table <- positives[,c(4,6)]
#have to hard code to go from tibble to table
prop.test(as.data.frame(prop.table))
View(prop_table)
prop_table$n_notpos
table(prop_table$n,prop_table$n_notpos)
table(prop_table$n[1],prop_table$n[2],prop_table$n_notpos[1],prop_table$n_notpos[2])
matrix(prop_table$n[1],prop_table$n[2],prop_table$n_notpos[1],prop_table$n_notpos[2])
matrix(prop_table$n[1],prop_table$n[2],prop_table$n_notpos[1],prop_table$n_notpos[2],ncol=2)
matrix(c(prop_table$n[1],prop_table$n[2],prop_table$n_notpos[1],prop_table$n_notpos[2]),ncol=2)
#Statistical analysis
positives$n_notpos <- positives$case_total-positives$n
prop_table <- positives[,c(4,6)]
prop_table <- matrix(c(prop_table$n[1],prop_table$n[2],prop_table$n_notpos[1],prop_table$n_notpos[2]),ncol=2)
#have to hard code to go from tibble to table
prop.test(prop_table)
View(positives)
#Statistical analysis
# We want to create a table for use in a prop.test
create_table <- function(tib) {
tib$n_not <- tib$case_total-tib$n
prop_table <- tib[,c(4,6)]
prop_table <- matrix(c(prop_table$n[1],prop_table$n[2],prop_table$n_not[1],prop_table$n_not[2]),ncol=2)
colnames(prop_table) <- c("pos","not pos")
rownames(prop_table) <- c("lower","upper")
return(prop_table)
=======
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_data_caps.rda")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#where do the dictionaries differ?
compare %>% filter(sentiment.bing != sentiment.nrc) %>%
select(word, sentiment.bing, sentiment.nrc) %>%
group_by(word) %>%
mutate("Occurences" = n()) %>%
slice(1) %>%
#examine "tough"
ids = bing.sents %>%
filter(word == "tough") %>%
select(Article_ID)
NYTimes %>%
filter(Article_ID %in% unlist(ids)) %>%
select(Title)
ungroup()
>>>>>>> 55ffe7c28c4276ec44d778032e9c8500ad18663e
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_data_caps.rda")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#logistic model for topic
#get most frequent words
most.freq <- words$word %>%
table()%>%
as_tibble() %>%
arrange(desc(n)) %>%
slice(1:30)
model.words <- most.freq$.
#create feature that counts word appearances
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
or(START, SPC) %R% feature.word %R% or(END, SPC))
}
}
#train classifier
model <- multinom(as.factor(Topic.Code) ~ . - Article_ID - Date - Title - Subject, data = model.data)
#training set accuracy
accuracy=mean(predict(model, newdata = model.data)==model.data$Topic.Code)
install.packages("rebus")
install.packages("rebus")
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_data_caps.rda")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#logistic model for topic
#get most frequent words
most.freq <- words$word %>%
table()%>%
as_tibble() %>%
arrange(desc(n)) %>%
slice(1:30)
model.words <- most.freq$.
#create feature that counts word appearances
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
or(START, SPC) %R% feature.word %R% or(END, SPC))
}
}
#train classifier
model <- multinom(as.factor(Topic.Code) ~ . - Article_ID - Date - Title - Subject, data = model.data)
#training set accuracy
accuracy=mean(predict(model, newdata = model.data)==model.data$Topic.Code)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_data_caps.rda")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#logistic model for topic
#get most frequent words
most.freq <- words$word %>%
table()%>%
as_tibble() %>%
arrange(desc(n)) %>%
slice(1:30)
model.words <- most.freq$.
#create feature that counts word appearances
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
(START%|%SPC) %R% feature.word %R% (END%|%SPC))
}
}
#train classifier
model <- multinom(as.factor(Topic.Code) ~ . - Article_ID - Date - Title - Subject, data = model.data)
#training set accuracy
accuracy=mean(predict(model, newdata = model.data)==model.data$Topic.Code)
accuracy
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_data_caps.rda")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#logistic model for topic
#get most frequent words
most.freq <- words$word %>%
table()%>%
as_tibble() %>%
arrange(desc(n)) %>%
slice(1:30)
model.words <- most.freq$.
#create feature that counts word appearances
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
(START%|%SPC) %R% feature.word %R% (END%|%SPC))
}
}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
(library(magrittr)
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
library(magrittr)
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
library(RTextTools)
library(dplyr)
library(tidytext)
library(ggplot2)
library(caTools)
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
library(RTextTools)
library(dplyr)
library(tidytext)
library(ggplot2)
library(caTools)
library(stringr)
library(rebus)
library(foreign)
library(nnet)
library(magrittr)
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
source("brian_analysis.R")
suppressMessages(suppressWarnings(library(RTextTools)))
source('~/php2560/inclass1018/text-mining-in-class-teamm/brian_analysis.R')
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_data_caps.rda")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#logistic model for topic
#get most frequent words
most.freq <- words$word %>%
table()%>%
as_tibble() %>%
arrange(desc(n)) %>%
slice(1:30)
model.words <- most.freq$.
#create feature that counts word appearances
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
(START%|%SPC) %R% feature.word %R% (END%|%SPC))
}
}
#train classifier
model <- multinom(as.factor(Topic.Code) ~ . - Article_ID - Date - Title - Subject, data = model.data)
#training set accuracy
accuracy=mean(predict(model, newdata = model.data)==model.data$Topic.Code)
library(RTextTools)
library(dplyr)
library(tidytext)
library(ggplot2)
library(caTools)
library(stringr)
library(rebus)
library(foreign)
library(nnet)
library(magrittr)
load("data/nyt_data_caps.rda")
NYTimes
words
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
View(words)
str(words)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
set.seed(23)
data("NYTimes")
NYTimes$Title = as.character(NYTimes$Title)
words = unnest_tokens(NYTimes, word, Title)
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
str(words)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_data_caps.rda")
source("clean_data.R")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#logistic model for topic
#get most frequent words
most.freq <- words$word %>%
table()%>%
as_tibble() %>%
arrange(desc(n)) %>%
slice(1:30)
model.words <- most.freq$.
#create feature that counts word appearances
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
(START%|%SPC) %R% feature.word %R% (END%|%SPC))
>>>>>>> 8b60cc68fe6872cac32e5e8ecb8fcbc903d617e3
}
}
source('~/php2560/inclass1018/text-mining-in-class-teamm/get_data.R')
source('~/php2560/inclass1018/text-mining-in-class-teamm/clean_data.R')
str(words)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
set.seed(23)
load("data/nyt_words.rda")
#clean/match
nrc.sents = inner_join(words, get_sentiments("nrc"), by="word")
bing.sents = inner_join(words, get_sentiments("bing"), by="word")
bing.words <- bing.sents %>% select(word)
nrc.words <- nrc.sents %>% filter(sentiment %in% c("positive", "negative")) %>% select(word)
#The above procedure is done to ensure that each token is counted only once. As described below,
#NRC gives a positive/negative value exactly once for each word.
num.bing=dim(bing.words)[1] #number of bing matches
num.nrc= dim(nrc.words)[1] #number of nrc matches
nrc.lengths <- apply(nrc.words, 1, nchar) #length of each word
bing.lengths <- apply(nrc.words, 1, nchar) #length of each word
df <- data.frame(length=c(nrc.lengths, bing.lengths), dict = c(rep("NRC", length(nrc.words)), rep("Bing", length(bing.words))))
ggplot(df, aes(x=factor(dict),y=length,fill=factor(dict)))+
geom_boxplot() + labs(title="Word Lengths by Dictionary") +facet_wrap(~dict)
dev.copy(png, 'graph/brianp1.png')
dev.off()
nrc.binary <- filter(nrc.sents, sentiment %in% c("positive", "negative")) #just looking at positive/negative
compare <- inner_join(bing.sents, nrc.binary,
by=c("word", "Article_ID"), suffix = c(".bing", ".nrc")) #words that match both
agreement = mean(compare$sentiment.bing==compare$sentiment.nrc)
#logistic model for topic
#get most frequent words
most.freq <- words$word %>%
table()%>%
as_tibble() %>%
arrange(desc(n)) %>%
slice(1:30)
model.words <- most.freq$.
#create feature that counts word appearances
model.data = NYTimes
for(i in 1:length(model.words)){
feature.word = model.words[i]
for(j in 1:dim(NYTimes)[1]){
model.data[[feature.word]][j]=str_count(NYTimes$Title[j],
(START%|%SPC) %R% feature.word %R% (END%|%SPC))
}
}
<<<<<<< HEAD
source('~/php2560/inclass1018/text-mining-in-class-teamm/get_data.R')
source('~/php2560/inclass1018/text-mining-in-class-teamm/clean_data.R')
rmarkdown::render("brian_paper.Rmd", output_format = "html_document")
source("get_data.R")
source("clean_data.R")
source("brian_analysis.R")
source("brian_analysis.R")
=======
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
(library(magrittr)
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings(library(RTextTools)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidytext)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
library(magrittr)
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
library(RTextTools)
library(dplyr)
library(tidytext)
library(ggplot2)
library(caTools)
suppressMessages(suppressWarnings(library(stringr)))
suppressMessages(suppressWarnings(library(rebus)))
suppressMessages(suppressWarnings(library(foreign)))
suppressMessages(suppressWarnings(library(nnet)))
suppressMessages(suppressWarnings(library(magrittr)))
#source("brian_analysis.R")
options(digits=3)
knitr::opts_chunk$set(echo = TRUE)
library(RTextTools)
library(dplyr)
library(tidytext)
library(ggplot2)
<<<<<<< HEAD
levels(sents.nrc$caps) <- c("Normal Capitalization", "All Caps")
sents.nrc %>%
filter(sentiment=="negative") %>%
count(word,caps) %>%
group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, caps, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=word,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(sentiment=="positive") %>%
count(word,caps) %>%
group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, caps, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=word,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
levents(sents.nrc$caps)
levels(sents.nrc$caps)
#Now comparing analysis by case
positives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="positive")
negatives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="negative")
surprises <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="surprise")
positives
negatives
surprises
#Now comparing analysis by case
positives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="positive")
negatives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="negative")
surprises <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="fear")
positives
negatives
surprises
#Now comparing analysis by case
positives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="positive")
negatives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="negative")
surprises <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="anger")
positives
negatives
surprises
#Now comparing analysis by case
positives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="positive")
negatives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="negative")
surprises <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="trust")
positives
negatives
surprises
#Now comparing analysis by case
positives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="positive")
negatives <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="negative")
surprises <- sents.nrc %>%
count(caps,sentiment,case_total) %>%
mutate(percent = n/case_total) %>%
filter(sentiment=="joy")
positives
negatives
surprises
sents.nrc %>%
count(word,caps) %>%
group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, caps, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=word,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(caps=="FALSE") %>%
count(word,sentiment) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=word,y=n,fill=sentiment)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(caps=="FALSE") %>%
count(word,sentiment) %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=word,y=n,fill=sentiment)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ sentiment, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(caps=="FALSE") %>%
count(caps,sentiment) %>%
group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(caps=="FALSE") %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(caps=="FALSE") %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
#facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(caps=="FALSE") %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
#mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
#facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
filter(caps=="FALSE") %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
#mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
#facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
#filter(caps=="FALSE") %>%
group_by(caps) %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
#mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
#filter(caps=="FALSE") %>%
group_by(caps) %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
#mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n)) +
geom_col(show.legend = FALSE,fill=caps) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
#filter(caps=="FALSE") %>%
group_by(caps) %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(10) %>%
ungroup() %>%
#mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
sents.nrc %>%
#filter(caps=="FALSE") %>%
group_by(caps) %>%
count(sentiment) %>%
# group_by(caps) %>%
top_n(15) %>%
ungroup() %>%
#mutate(word = reorder(paste(word, sentiment, sep = "__"), n)) %>%
# Set up the plot with aes()
ggplot(aes(x=sentiment,y=n,fill=caps)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
facet_wrap(~ caps, nrow = 2, scales = "free") +
coord_flip()
unlink("data", recursive = TRUE) # where data is stored after clean
unlink("olivia_results",recursive=TRUE)
unlink("graph", recursive = TRUE) # where graphs are stores
dir.create(file.path("data"), showWarnings = FALSE)
dir.create(file.path("olivia_results"), showWarnings = FALSE)
dir.create(file.path("graph"), showWarnings = FALSE)
source("get_data.R")
source("clean_data.R")
source("get_data.R")
source("clean_data.R")
source("olivia_analysis.R")
source("clean_data.R")
source("olivia_analysis.R")
load("~/Documents/PHP2560/inclass/reproducible/data/nyt_data.rda")
unlink("data", recursive = TRUE) # where data is stored after clean
unlink("olivia_results",recursive=TRUE)
unlink("graph", recursive = TRUE) # where graphs are stores
dir.create(file.path("data"), showWarnings = FALSE)
dir.create(file.path("olivia_results"), showWarnings = FALSE)
dir.create(file.path("graph"), showWarnings = FALSE)
source("get_data.R")
source("clean_data.R")
source("olivia_analysis.R")
source("linde-analysis.R")
source("olivia_vis.R")
rmarkdown::render("olivia_paper.Rmd", output_format = "html_document")
unlink("data", recursive = TRUE) # where data is stored after clean
unlink("olivia_results",recursive=TRUE)
unlink("graph", recursive = TRUE) # where graphs are stores
load("data/nyt_data.rda")
unlink("data", recursive = TRUE) # where data is stored after clean
unlink("olivia_results",recursive=TRUE)
unlink("graph", recursive = TRUE) # where graphs are stores
unlink("linde_graph", recursive = TRUE) # where graphs are stores
dir.create(file.path("data"), showWarnings = FALSE)
dir.create(file.path("olivia_results"), showWarnings = FALSE)
dir.create(file.path("graph"), showWarnings = FALSE)
dir.create(file.path("linde_graphs"), showWarnings = FALSE)
source("get_data.R")
source("clean_data.R")
source("olivia_analysis.R")
source("linde-analysis.R")
source("olivia_vis.R")
rmarkdown::render("olivia_paper.Rmd", output_format = "html_document")
rmarkdown::render("olivia_paper.Rmd", output_format = "html_document")
rmarkdown::render("linde_paper.Rmd", output_format = "html_document")
unlink("data", recursive = TRUE) # where data is stored after clean
unlink("olivia_results",recursive=TRUE)
unlink("graph", recursive = TRUE) # where graphs are stores
unlink("linde_graph", recursive = TRUE) # where graphs are stores
unlink("data", recursive = TRUE) # where data is stored after clean
unlink("olivia_results",recursive=TRUE)
unlink("graph", recursive = TRUE) # where graphs are stores
unlink("linde_graphs", recursive = TRUE) # where graphs are stores
dir.create(file.path("data"), showWarnings = FALSE)
dir.create(file.path("olivia_results"), showWarnings = FALSE)
dir.create(file.path("graph"), showWarnings = FALSE)
dir.create(file.path("linde_graphs"), showWarnings = FALSE)
source("get_data.R")
source("clean_data.R")
source("olivia_analysis.R")
source("linde-analysis.R")
source("olivia_vis.R")
rmarkdown::render("olivia_paper.Rmd", output_format = "html_document")
unlink("data", recursive = TRUE) # where data is stored after clean
unlink("olivia_results",recursive=TRUE)
unlink("graph", recursive = TRUE) # where graphs are stores
unlink("linde_graphs", recursive = TRUE) # where graphs are stores
=======
library(caTools)
library(stringr)
library(rebus)
library(foreign)
library(nnet)
library(magrittr)
#source("brian_analysis.R")
options(digits=3)
>>>>>>> 8b60cc68fe6872cac32e5e8ecb8fcbc903d617e3
>>>>>>> 55ffe7c28c4276ec44d778032e9c8500ad18663e
